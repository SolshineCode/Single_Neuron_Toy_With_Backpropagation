import math
import numpy as np
import matplotlib
matplotlib.use('TkAgg')  # Use Tkinter for rendering
import matplotlib.pyplot as plt


def train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):
	
	# Make sure features and labels are arrays
	features = np.array(features)
	labels = np.array(labels)
 
	#Neuron inference:
	def inference(weights, features, bias):
		#weights dot features
		weightfeatures = np.dot(features,weights)
		#plus bias
		weightfeaturesbiased = np.add(weightfeatures,bias)
		#sigmoid it
		generation = 1 / (1 + np.exp(-weightfeaturesbiased))
		return generation
	
	#set neuron parameters ahead of time to intials, pretraining
	weights = np.array(initial_weights)
	bias = np.array(initial_bias)
	
	#BackProp:
 	# Here we could divide the dataset into batches
 
	# Initialize lists
	mse_values = []  # mse empty list initialization
	grad_history = []  # Track gradients
	activation_history = []  # Track activations

	# Loop through each epoch
	while epochs > 0:
		#Run inference
		generation = inference(weights, features, bias)  # Store the output of inference
		activation_history.append(generation.tolist())  # Log activations for visualization
        
		#Calculate MSE
		Errorrate = np.subtract(generation, labels)
		MSE = np.square(Errorrate).mean()
		mse_values.append(MSE)  # Append MSE to the list
  
		# Backprop modify weights and bias
		learning_rate = 0.01  # Define a learning rate
		# Calculate gradients
		gradients = np.dot(features.T, (generation - labels)) / len(labels)  # Gradient for weights
		grad_history.append(gradients.tolist())  # Append current gradients cause we're going to visualize them
		weights -= learning_rate * gradients  # Update weights
		bias -= learning_rate * np.mean(generation - labels)  # Update bias
		epochs = epochs - 1
  
	# Visualizations
 
	# Gradient Dynamics
	grad_history = np.array(grad_history)
	for i in range(grad_history.shape[1]):
		plt.plot(range(len(grad_history)), grad_history[:, i], label=f"Feature {i+1}")
	plt.xlabel("Epoch")
	plt.ylabel("Gradient Value")
	plt.title("Gradient Dynamics Over Training")
	plt.legend()
	plt.show()

	# Take final arrays out to just be lists		
	updated_weights = weights
	updated_bias = bias
	updated_weights = np.around(updated_weights, decimals = 4).tolist()
	updated_bias = np.around(updated_bias, decimals = 4).tolist()
	mse_values = np.around(mse_values, decimals = 4).tolist()

	return updated_weights, updated_bias, mse_values


# Test case
updated_weights, updated_bias, mse_values = train_neuron(
    features=np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), 
    labels=np.array([1, 0, 0]),
    initial_weights=[0.1, -0.2],
    initial_bias=0.0,
    learning_rate=0.1,
    epochs=2
)
print("Test case 1 passed!")
print(f"Updated weights: {updated_weights}")
print(f"Updated biases: {updated_bias}")
print(f"MSE values: {mse_values}")


# Test case without arraying the inputs first 
updated_weights, updated_bias, mse_values = train_neuron(
    features=[[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], 
    labels=[1, 0, 0],
    initial_weights=[0.1, -0.2],
    initial_bias=0.0,
    learning_rate=0.1,
    epochs=2
)
print("Test case 2 passed!")
print(f"Updated weights: {updated_weights}")
print(f"Updated biases: {updated_bias}")
print(f"MSE values: {mse_values}")


# Test case without arraying the inputs first 
updated_weights, updated_bias, mse_values = train_neuron(
    features=[[2.0, 2.0], [2.0, 2.0], [-1.0, -2.0]], 
    labels=[1, 1, 0],
    initial_weights=[0.3, -0.2],
    initial_bias=0.0,
    learning_rate=0.1,
    epochs=6
)
print("Test case 3 passed!")
print(f"Updated weights: {updated_weights}")
print(f"Updated biases: {updated_bias}")
print(f"MSE values: {mse_values}")
